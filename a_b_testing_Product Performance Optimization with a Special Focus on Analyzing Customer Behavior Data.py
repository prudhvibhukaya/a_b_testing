# -*- coding: utf-8 -*-
"""A/B Testing Thesis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BMImlvzgrlvZ2sy2D3Wgr2wZKEZ7Uqqh
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import random
import matplotlib.pyplot as plt
# %matplotlib inline
from sklearn.linear_model import LogisticRegression
import statsmodels.api as sm
from scipy.stats import norm

df = pd.read_csv("/content/ab_data.csv")
df.head()

df.user_id.nunique()
df.groupby(['group','landing_page']).count()['converted']

all_conv = "%.4f" % (df.converted.sum() / df.converted.count())

control_conv = df.query("group == 'control'")['converted'].sum() / df.query("group == 'control'")['converted'].count()
"%.4f" % control_conv

treat_conv = df.query("group == 'treatment'")['converted'].sum() / df.query("group == 'treatment'")['converted'].count()
"%.4f" % treat_conv

p_n = df.query('landing_page == "new_page"').user_id.nunique() / df.user_id.nunique()
"%.4f" % p_n

n_new = df.query('landing_page == "new_page" ').count()[0]

n_old = df.query('landing_page == "old_page" ').count()[0]

144226

new_converted_simulation = np.random.binomial(n_new, treat_conv, 20000)/n_new

old_converted_simulation = np.random.binomial(n_old, control_conv, 20000)/n_old

p_diffs = new_converted_simulation - old_converted_simulation

p_diffs = np.array(p_diffs)
plt.hist(p_diffs)

new_convert = df.query('converted == 1 and landing_page == "new_page"').count()[0]/n_new

old_convert = df.query('converted == 1 and landing_page == "old_page"').count()[0]/n_old

obs_diff = new_convert - old_convert

null_vals = np.random.normal(0, p_diffs.std(), p_diffs.size)
plt.hist(null_vals);
plt.axvline(x=obs_diff, color='red')

(null_vals > obs_diff).mean()

convert_old = df.query('converted == 1 and landing_page == "old_page"').count()[0]

convert_new = df.query('converted == 1 and landing_page == "new_page"').count()[0]

n_old = df.query('landing_page == "old_page" ').count()[0]
n_new = df.query('landing_page == "new_page" ').count()[0]

convert_old,convert_new,n_old,n_new

z_score, p_value = sm.stats.proportions_ztest(np.array([convert_new,convert_old]), np.array([n_new,n_old]), alternative = 'larger')

z_score, p_value

norm.ppf(1-(0.05/2))

norm.cdf(z_score)

df['intercept'] = 1

df[['ab_page','old_page']] = pd.get_dummies(df['landing_page'])

df.head ()

df = df.drop('old_page', axis = 1)

log = sm.Logit(df['converted'], df[['intercept', 'ab_page']])
results = log.fit()
results.summary()

import statsmodels.api as sm
import numpy as np

convert_old, convert_new, n_old, n_new

p_old = convert_old / n_old
p_new = convert_new / n_new

se_old = np.sqrt(p_old * (1 - p_old) / n_old)
se_new = np.sqrt(p_new * (1 - p_new) / n_new)

z_score = 1.96

margin_error_old = z_score * se_old
margin_error_new = z_score * se_new

ci_old = (p_old - margin_error_old, p_old + margin_error_old)
ci_new = (p_new - margin_error_new, p_new + margin_error_new)

print("Confidence Interval for Old Page:", ci_old)
print("Confidence Interval for New Page:", ci_new)

import matplotlib.pyplot as plt

# Confidence Intervals
old_page_ci = (0.1188, 0.1221)
new_page_ci = (0.1172, 0.1205)

# Page types
page_types = ['Old Page', 'New Page']

# Lower bounds
lower_bounds = [old_page_ci[0], new_page_ci[0]]

# Upper bounds
upper_bounds = [old_page_ci[1], new_page_ci[1]]

# Create horizontal bar chart
fig, ax = plt.subplots(figsize=(10, 6))

# Plot confidence intervals
ax.barh(page_types, upper_bounds, color='lightblue', label='Upper Bound')
ax.barh(page_types, lower_bounds, color='skyblue', label='Lower Bound')

# Highlight the point estimate
ax.scatter([0.1205, 0.1172], page_types, color='red', marker='o', label='Point Estimate')

# Add labels and title
ax.set_xlabel('Conversion Rate')
ax.set_title('Confidence Intervals for Old and New Pages')

# Add legend
ax.legend()

# Display the chart
plt.show()

import matplotlib.pyplot as plt

# Confidence Intervals
ci_old = (0.11881486276627486, 0.12214031894509238)
ci_new = (0.11718786569635213, 0.12049372681650113)

# Plotting
plt.bar(['Old Page', 'New Page'], [ci_old[1], ci_new[1]], yerr=[[ci_old[1]-p_old, ci_new[1]-p_new]], capsize=10, color=['lightblue', 'lightgreen'])
plt.title('Conversion Rate with 95% Confidence Interval')
plt.ylabel('Conversion Rate')
plt.show()

# Conducting the z-test
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative='larger')

import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import norm

# Observed z-score and p-value from the z-test
observed_z = 1.29  # Replace with your actual observed z-score
p_value = 0.9143962454534289  # Replace with your actual p-value

# Plotting the standard normal distribution
x = np.linspace(-3, 3, 1000)
y = norm.pdf(x, 0, 1)

plt.plot(x, y, label='Standard Normal Distribution')

# Highlighting the critical region (right side for one-tailed test)
critical_region = x[x > observed_z]
plt.fill_between(critical_region, norm.pdf(critical_region, 0, 1), color='red', alpha=0.5, label='Critical Region')

# Marking the observed z-score
plt.axvline(x=observed_z, color='blue', linestyle='--', label='Observed z-score')

# Adding labels and legend
plt.title('Z-Test Results')
plt.xlabel('Z-Score')
plt.ylabel('Probability Density Function')
plt.legend()

# Displaying the plot
plt.show()